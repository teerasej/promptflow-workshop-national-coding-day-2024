{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Evaluation with Data\n",
    "In this notebook, we introduce built-in evaluators and guide you through creating your own custom evaluators. We'll cover both code-based and prompt-based custom evaluators. Finally, we'll demonstrate how to use the `evaluate` API to assess data using these evaluators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da43d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to set the environment variables if not already set. If set, you can skip this step.\n",
    "import os\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"<api_key>\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"<api_version>\"\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"<deployment>\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"<endpoint>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcb5a0",
   "metadata": {},
   "source": [
    "## 1. Built-in Evaluators\n",
    "\n",
    "The table below lists all the built-in evaluators we support. In the following sections, we will select a few of these evaluators to demonstrate how to use them.\n",
    "\n",
    "| Category       | Namespace                                        | Evaluator Class           | Notes                                             |\n",
    "|----------------|--------------------------------------------------|---------------------------|---------------------------------------------------|\n",
    "| Quality        | promptflow.evals.evaluators                      | GroundednessEvaluator     |                                                   |\n",
    "|                |                                                  | RelevanceEvaluator        |                                                   |\n",
    "|                |                                                  | CoherenceEvaluator        |                                                   |\n",
    "|                |                                                  | FluencyEvaluator          |                                                   |\n",
    "|                |                                                  | SimilarityEvaluator       |                                                   |\n",
    "|                |                                                  | F1ScoreEvaluator          |                                                   |\n",
    "| Content Safety | promptflow.evals.evaluators.content_safety       | ViolenceEvaluator         |                                                   |\n",
    "|                |                                                  | SexualEvaluator           |                                                   |\n",
    "|                |                                                  | SelfHarmEvaluator         |                                                   |\n",
    "|                |                                                  | HateUnfairnessEvaluator   |                                                   |\n",
    "| Composite      | promptflow.evals.evaluators                      | QAEvaluator               | Built on top of individual quality evaluators.    |\n",
    "|                |                                                  | ChatEvaluator             | Similar to QAEvaluator but designed for evaluating chat messages. |\n",
    "|                |                                                  | ContentSafetyEvaluator    | Built on top of individual content safety evaluators. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f72d3d",
   "metadata": {},
   "source": [
    "### 1.1 Quality Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import RelevanceEvaluator\n",
    "\n",
    "# Initialzing Relevance Evaluator\n",
    "relevance_eval = RelevanceEvaluator(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf075450-1835-4b15-8002-76ae249caab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Relevance Evaluator on single input row\n",
    "relevance_score = relevance_eval(\n",
    "    answer=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    context=\"From the our product list,\"\n",
    "    \" the alpine explorer tent is the most waterproof.\"\n",
    "    \" The Adventure Dining Table has higher weight.\",\n",
    "    question=\"Which tent is the most waterproof?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67844e07-57bb-415f-b1af-606b5a67aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdd503",
   "metadata": {},
   "source": [
    "### 1.2 Content Safety Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ab346",
   "metadata": {},
   "source": [
    "Unlike quality evaluators, which are prompt-based, content safety evaluators are supported by the RAI service. To use these evaluators, you need to provide the subscription ID, resource group, and project name of your Azure AI project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c304e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Project Scope\n",
    "project_scope = {\n",
    "    \"subscription_id\": \"<subscription_id>\",\n",
    "    \"resource_group_name\": \"<resource_group_name>\",\n",
    "    \"project_name\": \"<project_name>\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators.content_safety import ViolenceEvaluator\n",
    "\n",
    "# Initialzing Violence Evaluator\n",
    "violence_eval = ViolenceEvaluator(project_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be423024-4725-4f19-a5e1-191a01066860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Violence Evaluator on single input row\n",
    "violence_score = violence_eval(question=\"What is the capital of France?\", answer=\"Paris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18aa4a-78fb-45b3-9d1b-8babcd9ab931",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(violence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8e0ae",
   "metadata": {},
   "source": [
    "## 2. Custom Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f07695",
   "metadata": {},
   "source": [
    "After gaining a basic understanding of how the built-in evaluators work, let's explore how to create your own evaluator. In this section, we will discuss creating both a code-based custom evaluator and a prompt-based custom evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac30330",
   "metadata": {},
   "source": [
    "### 2.1 Define a Code based Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10faf0",
   "metadata": {},
   "source": [
    "A code-based evaluator can be as simple as a callable class. In the example below, we will show you a simple evaluator that calculates the length of an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf976514",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"custom/answer_length.py\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86568cf3-15ff-43b2-b80d-bb3671f557ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.answer_length import AnswerLengthEvaluator\n",
    "\n",
    "answer_length = AnswerLengthEvaluator()\n",
    "\n",
    "print(answer_length(answer=\"some answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271d78c-ade7-4eb7-85e1-cba9e5c4ad53",
   "metadata": {},
   "source": [
    "### 2.2 Define a Prompty based Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fedd4",
   "metadata": {},
   "source": [
    "Prompty is a file with .prompty extension for developing prompt template. The prompty asset is a markdown file with a modified front matter. The front matter is in yaml format that contains a number of metadata fields which defines model configuration and expected inputs of the prompty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"custom/apology.prompty\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344d8ff-7ef8-4042-ac70-e3f3bc4db5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import load_flow\n",
    "\n",
    "# load apology evaluatorfrom prompty\n",
    "apology_eval = load_flow(source=\"custom/apology.prompty\", model={\"configuration\": model_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = apology_eval(\n",
    "    question=\"What is the capital of France?\", answer=\"Paris\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9019d37-a323-4c98-9c55-a7acb72d2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "## 3. Using Evaluate API to evaluate with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe3fa2",
   "metadata": {},
   "source": [
    "In previous sections, we walked you through how to use built-in evaluators to evaluate a single row and how to define your own custom evaluators. Now, we will show you how to use these evaluators with the powerful `evaluate` API to assess an entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2265e",
   "metadata": {},
   "source": [
    "First, let's take a peek at what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"data.jsonl\"\n",
    "\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcfc57",
   "metadata": {},
   "source": [
    "Now, we will invoke the `evaluate` API using a few evaluators that we already initialized\n",
    "\n",
    "Additionally, we have a column mapping to map the `truth` column from the dataset to `ground_truth`, which is accepted by the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\",\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"violence\": violence_eval,\n",
    "        \"answer_length\": answer_length,\n",
    "        \"apology\": apology_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"ground_truth\": \"${data.truth}\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0fb00",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's check the results produced by the evaluate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dadc4b-495f-4a46-ad15-d4c8a910b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, JSON\n",
    "\n",
    "display(JSON(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb28f9a-1f9c-4c2e-8b32-e7da51585f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results using Azure AI Studio UI\n",
    "print(result[\"studio_url\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
