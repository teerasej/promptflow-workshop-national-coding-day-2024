{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Upload of evaluators\n",
    "In this notebook we are demonstrating the upload of the standard evaluators.\n",
    "\n",
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e21ceb-1e58-4ba7-884a-5e103aea7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import uuid\n",
    "import yaml\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import (\n",
    "    Model\n",
    ")\n",
    "\n",
    "from promptflow.client import PFClient\n",
    "from promptflow.evals.evaluate import evaluate\n",
    "from promptflow.evals.evaluators import F1ScoreEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "## End to end demonstration of evaluator saving and uploading to Azure.\n",
    "### Saving the standard evaluators to the flex format.\n",
    "First we will create the promptflow client, which will be used to save the existing flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PFClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fcaf38-6caa-4c1b-ada5-479686232cd1",
   "metadata": {},
   "source": [
    "We will use F1 score evaluator from the standard evaluator set and save it to local directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fae04d-f6d0-4cc3-b149-a6058158c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.flows.save(F1ScoreEvaluator, path='./f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03552c5a-3ecc-4154-a0bd-e3fe2831e323",
   "metadata": {},
   "source": [
    "Let us inspect, what has been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27602c-e2c8-49c2-8d00-1eb5a11e55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(os.listdir('f1_score')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4dbfe-faa0-44f9-a53e-310c946d91fe",
   "metadata": {},
   "source": [
    "The file, defining entrypoint of our model is called flow.flex.yaml, let us display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30c3e3-8113-4e0f-9210-b062e7354099",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('f1_score', 'flow.flex.yaml')) as fp:\n",
    "    flex_definition = yaml.safe_load(fp)\n",
    "print(f\"The evaluator entrypoint is {flex_definition['entry']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb9a12-8bdd-4679-9957-998f3c7ceb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PFClient()\n",
    "run = pf.run(\n",
    "    flow='f1_score',\n",
    "    data='data.jsonl',\n",
    "    name=f'test_{uuid.uuid1()}',\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fbd6bf-6977-457b-aa55-f6dad9ec1f73",
   "metadata": {},
   "source": [
    "Now let us test the flow with the simple dataset, consisting of one ground true and one actual sentense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73f698-7cab-4b30-8947-411c2060560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    \"ground_truth\": [\"January is the coldest winter month.\"],\n",
    "    \"answer\": [\"June is the coldest summer month.\"]\n",
    "})\n",
    "in_file = 'sample_data.jsonl'\n",
    "data.to_json('sample_data.jsonl', orient='records', lines=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3589c8-0df8-489f-b5a8-beb5ae2aec6a",
   "metadata": {},
   "source": [
    "Load the evaluator in a FLEX format and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841d6109-23b9-45c5-b709-b588f932f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_result = pf.test(flow='f1_score', inputs='sample_data.jsonl')\n",
    "print(f\"Flow outputs: {flow_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94aab5-73f8-4c7d-a7e0-a92853db0198",
   "metadata": {},
   "source": [
    "Now we have all the tools to upload our model to Azure\n",
    "### Uploading data to Azure\n",
    "First we will need to authenticate to azure. For this purpose we will use the the configuration file of the net structure.\n",
    "```json\n",
    "{\n",
    "    \"resource_group_name\": \"resource-group-name\",\n",
    "    \"workspace_name\": \"ws-name\",\n",
    "    \"subscription_id\": \"subscription-uuid\",\n",
    "    \"registry_name\": \"registry-name\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188cb7d-d7c1-460f-9f3e-91546d8b8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as f:\n",
    "    configuration = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e6627-67d8-43c3-b491-e3a8802197b2",
   "metadata": {},
   "source": [
    "#### Uploading to the workspace\n",
    "In this scenario we will not need the `registry_name` in our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36338014-05f9-4f37-9fb0-726bb1c137b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ws = configuration.copy()\n",
    "del config_ws[\"registry_name\"]\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    **config_ws,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4a912-047f-4614-85a7-cfff86874303",
   "metadata": {},
   "source": [
    "We will use the evaluator operations API to upload our model to workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4edb19-f0b8-498c-908d-c7e23ba7b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = Model(\n",
    "    path=\"f1_score\",\n",
    "    name='F1Score-Evaluator',\n",
    "    description=\"Measures the ratio of the number of shared words between the model generation and the ground truth answers.\",\n",
    ")\n",
    "ml_client.evaluators.create_or_update(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6423eb6-415c-463c-839d-da0cf70bf245",
   "metadata": {},
   "source": [
    "Now we will retrieve model and check that it is functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18dd491-ee43-4dda-8a5b-d5317f8cb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.evaluators.download('F1Score-Evaluator', version='1', download_path='f1_score_downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15468f0-681a-49fe-a883-0da44f68293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_result = pf.test(flow=os.path.join('f1_score_downloaded', 'F1Score-Evaluator', 'f1_score'), inputs='data.jsonl')\n",
    "print(f\"Flow outputs: {flow_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0708cd5-66e7-46f2-a8d0-b41e82278a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('f1_score_downloaded')\n",
    "assert not os.path.isdir('f1_score_downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a4588-a0b7-4bd9-adc6-6595084da3b7",
   "metadata": {},
   "source": [
    "#### Uploading to the registry\n",
    "In this scenario we will not need the `workspace_name` in our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b57845-77f0-4a2e-9b2f-ccb3fb825da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_reg = configuration.copy()\n",
    "del config_reg[\"workspace_name\"]\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    **config_reg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0f024-c6e5-4e51-aaaa-021eaa4c14c4",
   "metadata": {},
   "source": [
    "We are creating new eval here, because create_or_update changes the model inplace, adding non existing link to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488f4ff-b97f-43e6-82ab-a78a2e9e2da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = Model(\n",
    "    path=\"f1_score\",\n",
    "    name='F1Score-Evaluator',\n",
    "    description=\"Measures the ratio of the number of shared words between the model generation and the ground truth answers.\",\n",
    "    properties={\"show-artifact\": \"true\"}\n",
    ")\n",
    "ml_client.evaluators.create_or_update(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6603c7d-7eaf-454e-a59a-2dd01fd3afc6",
   "metadata": {},
   "source": [
    "Now we will perform the same sanity check, we have done for the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc3d3b-db1a-4a5a-97c5-4ff701051695",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.evaluators.download('F1Score-Evaluator', version='1', download_path='f1_score_downloaded')\n",
    "flow_result = pf.test(flow=os.path.join('f1_score_downloaded', 'F1Score-Evaluator', 'f1_score'), inputs='data.jsonl')\n",
    "print(f\"Flow outputs: {flow_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167df44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import Flow\n",
    "\n",
    "# This is not working but it should. Will uncomment once PF team provides a fix.\n",
    "# f = Flow.load('f1_score_downloaded/F1Score-Evaluator/f1_score')\n",
    "# f(question='What is the capital of France?', answer='Paris', ground_truth='Paris is the capital of France.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5dc1c9-9c7a-40f7-9b23-9b30bedd5dd4",
   "metadata": {},
   "source": [
    "Finally, we will do the cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b52e8-1f46-454d-a6a5-2ad725e927fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('f1_score_downloaded')\n",
    "assert not os.path.isdir('f1_score_downloaded')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
