{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Loading and saving the evaluators.\n",
    "In this notebook we will generate evaluators, save them to model registry, download them and use in the `evaluate` call.\n",
    "\n",
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e21ceb-1e58-4ba7-884a-5e103aea7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import inspect\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.exceptions import UnsupportedOperationError\n",
    "from azure.identity._credentials.default import AzureCliCredential\n",
    "\n",
    "from promptflow.client import PFClient, load_flow\n",
    "from promptflow.core import AzureOpenAIModelConfiguration, Flow\n",
    "from promptflow.evals.evaluate import evaluate\n",
    "from promptflow.evals.evaluators import F1ScoreEvaluator, GroundednessEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af34b82-c5c0-4bf3-acc1-c7984fe84c2d",
   "metadata": {},
   "source": [
    "## Create or load evaluators\n",
    "### Flex Flow evaluator\n",
    "First we will create the flex flow evaluator and will save it. It wil be just a function, which calculates the length of an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63574d24-3424-4b65-9c68-3839dc24d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_len(answer):\n",
    "    return len(answer)\n",
    "\n",
    "\n",
    "target_dir_tmp = \"flex_flow_tmp\"\n",
    "os.makedirs(target_dir_tmp, exist_ok=True)\n",
    "lines = inspect.getsource(answer_len)\n",
    "with open(os.path.join(\"flex_flow_tmp\", \"answer.py\"), \"w\") as fp:\n",
    "    fp.write(lines)\n",
    "\n",
    "from flex_flow_tmp.answer import answer_len as answer_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85313246-d13c-4c36-8c08-5cd192e0eea1",
   "metadata": {},
   "source": [
    "After we have created the function, we can save it as a flex flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa627b-a1f9-4e82-bdf6-ffdeeed04355",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PFClient()\n",
    "flex_flow_path = \"flex_flow\"\n",
    "if os.path.isdir(flex_flow_path):\n",
    "    shutil.rmtree(flex_flow_path)\n",
    "pf.flows.save(entry=answer_length, path=flex_flow_path)\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(target_dir_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b7e43-7da6-4037-8dbc-1d9c6ad46ccb",
   "metadata": {},
   "source": [
    "Now we will test the saved flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f5f6c-dfb6-4d78-a500-1a21ec4268fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_flex_flow = load_flow(flex_flow_path)\n",
    "type(saved_flex_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d10981-ee47-45bb-960e-892a5c8faedf",
   "metadata": {},
   "source": [
    "### DAG Evaluator\n",
    "Now we will load the DAG evaluator, which counts the number LLM apologises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99469856-c5a5-410c-b8b0-b2ceb6856f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_path = \"apology_dag\"\n",
    "dag_evaluator = load_flow(dag_path)\n",
    "print(type(dag_evaluator))\n",
    "dag_evaluator(answer=\"Sorry, I can only truth questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc98cb-d756-4ced-b8c1-2e51ef6ec438",
   "metadata": {},
   "source": [
    "### Prompty flow\n",
    "Fist we will need to set the authentication variables.\n",
    "In the `eval-basic\\eval.prompty` file make sure that `azure_deployment` is set to the value, equal to yours deployment.\n",
    "Please create the deployment of gpt-3.5 or gpt-4 in Azure Open AI and create the json file with the next contents.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"AZURE_OPENAI_API_KEY\": \"super_secret_key\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\": \"https:deployment_name.openai.azure.com/\"\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, we will load the prompty flow, which return 1 if the llm apologises and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01f86e-2591-4227-acf9-e1095899abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "secure_data = json.load(open(\"openai_auth.json\"))\n",
    "assert \"AZURE_OPENAI_API_KEY\" in secure_data\n",
    "assert \"AZURE_OPENAI_ENDPOINT\" in secure_data\n",
    "\n",
    "for k, v in secure_data.items():\n",
    "    os.environ[k] = v\n",
    "\n",
    "prompty_path = os.path.join(\"apology-prompty\", \"apology.prompty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b287b-7440-44aa-8b3e-b79860423dda",
   "metadata": {},
   "source": [
    "Now we will try the loaded flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b37187-d77f-4296-be47-cc78979185f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompty_flow = load_flow(prompty_path)\n",
    "results = evaluate(data=\"evaluation_dataset_context.jsonl\", evaluators={\"prompty_eval\": prompty_flow})\n",
    "pd.DataFrame(results[\"rows\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94aab5-73f8-4c7d-a7e0-a92853db0198",
   "metadata": {},
   "source": [
    "### Authenticate to Azure\n",
    "First we will need to authenticate to azure. For this purpose we will use the the configuration file of the net structure.\n",
    "```json\r\n",
    "{\r\n",
    "    \"resource_group_name\": \"resource-group-name\",\r\n",
    "    \"subscription_id\": \"subscription-uuid\",\r\n",
    "    \"registry_name\": \"registry-name\"\r\n",
    "}\r\n",
    "```\r\n",
    "**Note:** If the `registry_name` will be replaced by `workspace_name`, the evaluator will be saved to Azure ML Workspace instead of registry.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188cb7d-d7c1-460f-9f3e-91546d8b8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\") as f:\n",
    "    configuration = json.load(f)\n",
    "\n",
    "credential = AzureCliCredential()\n",
    "ml_client = MLClient(credential=credential, **configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e6627-67d8-43c3-b491-e3a8802197b2",
   "metadata": {},
   "source": [
    "Now we will upload all the evaluators to Azure.<br>\n",
    "FlexFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36338014-05f9-4f37-9fb0-726bb1c137b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = Model(\n",
    "    path=flex_flow_path,\n",
    "    name=\"answer_len_uploaded\",\n",
    "    description=\"Evaluator, calculating answer length using Flex flow.\",\n",
    ")\n",
    "flex_model = ml_client.evaluators.create_or_update(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f72ab9-f7f6-4fe4-8f10-c96f53ba3180",
   "metadata": {},
   "source": [
    "Upload DAG flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188164c8-6fb1-4caa-9800-e15c8c229163",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = Model(\n",
    "    path=dag_path,\n",
    "    name=\"apology_dag_uploaded\",\n",
    "    description=\"Evaluator, calculating the number of times apology happens in the answer using DAG flow.\",\n",
    ")\n",
    "dag_model = ml_client.evaluators.create_or_update(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16f77d-ca98-4305-9cff-7df4ee734a5a",
   "metadata": {},
   "source": [
    "Finally, let us upload the prompty model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055e735-1f7f-492b-9265-c2a54b130876",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = Model(\n",
    "    path=os.path.dirname(prompty_path),\n",
    "    name=\"apology_prompty_uploaded\",\n",
    "    description=\"Evaluator, showing, if apology happens in the response.\",\n",
    ")\n",
    "prompty_model = ml_client.evaluators.create_or_update(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3364a4-7238-4946-825e-e89b4ec7d64d",
   "metadata": {},
   "source": [
    "The registered evaluators can be retrieved from the registry or workspace using `get` method. It will return the model, which we have created in steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86e83c-7c05-4aec-a59c-9aa0fe4b18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_eval = ml_client.evaluators.get(\"apology_prompty_uploaded\", version=1)\n",
    "retrieved_eval.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7ea8a-9332-483e-bdfd-efe99af26e64",
   "metadata": {},
   "source": [
    "We can also list all the evaluators with the given name. As with the `ModelOperations` API, it will return an iterator of models, however in this case all models will be marked as evaluators. Let us iterate over the evaluators and print their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9712a5-b18e-481e-8b5f-5310cf04e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_list = [[eval.name, eval.version] for eval in ml_client.evaluators.list(\"apology_prompty_uploaded\")]\n",
    "pd.DataFrame(evals_list, columns=['Name', 'Version']).sort_values(by='Version')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce4d5a-82be-46d9-8a29-b7b116097d9a",
   "metadata": {},
   "source": [
    "**Limitation!** Please note, that unlike for models, currently we can not list evaluators without providing name and it will raise an `UnsupportedOperationError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc7ada2-c71c-4c16-a190-4a5042978e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ml_client.evaluators.list(None)\n",
    "except UnsupportedOperationError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6423eb6-415c-463c-839d-da0cf70bf245",
   "metadata": {},
   "source": [
    "Now we will download the evaluators and load them in promptflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18dd491-ee43-4dda-8a5b-d5317f8cb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {}\n",
    "for name, dirname, mod in zip(\n",
    "    (\"answer_len\", \"apology_number\", \"apopogy\"),\n",
    "    (flex_flow_path, dag_path, prompty_path),\n",
    "    (flex_model, dag_model, prompty_model),\n",
    "):\n",
    "    ml_client.evaluators.download(mod.name, version=mod.version, download_path=\".\")\n",
    "    evaluators[name] = load_flow(os.path.join(mod.name, dirname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e0ae9-c94b-4c88-9422-fc80f4699443",
   "metadata": {},
   "source": [
    "View the loaded evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55215e0-7f32-4c03-a88a-93dd749761b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce3228c-425b-45d3-a76c-1c9abc9e53b7",
   "metadata": {},
   "source": [
    "## Run evaluators using evalute API\n",
    "Let us run three loaded evaluators along with two standard evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1b381-dd17-4edb-bc1f-fc3804a6e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=secure_data[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=secure_data[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_deployment=\"gpt-35-turbo-1106\",  # Please use the name of a model you have deployed.\n",
    ")\n",
    "\n",
    "evaluators[\"f1_evaluator\"] = F1ScoreEvaluator()\n",
    "evaluators[\"groundedess_evaluator\"] = GroundednessEvaluator(model_config=configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93d2cb-40a6-4906-98e0-6f38008c68cc",
   "metadata": {},
   "source": [
    "Finally, run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8740be0-11af-49ff-b28d-2e4e56a82485",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(data=\"evaluation_dataset_context.jsonl\", evaluators=evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0569608-5db9-4865-8b5f-6c62fd2a3696",
   "metadata": {},
   "source": [
    "View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c984dec-ca03-4412-9705-0e3f4954bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{results['metrics']=}\")\n",
    "pd.DataFrame(results[\"rows\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_evals",
   "language": "python",
   "name": "test_evals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
